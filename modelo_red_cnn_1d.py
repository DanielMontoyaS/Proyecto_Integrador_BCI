# -*- coding: utf-8 -*-
"""Modelo_Red_CNN_1D.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ELDNSvWpFIXozLQVOKdytTdIumNyNkMc

# **Importe de librerias**
"""

from glob import glob
import os 
import numpy as np

import pandas as pd 

import matplotlib.pyplot as plt

import time

from statistics import mean

import seaborn as sns

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GroupKFold, LeaveOneGroupOut
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold
from sklearn.utils import shuffle
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import confusion_matrix

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Conv1D,BatchNormalization,LeakyReLU,MaxPool1D,\
GlobalAveragePooling1D,Dense,Dropout,AveragePooling1D,MaxPooling1D,Flatten,Activation
from tensorflow.keras.models import Sequential
from tensorflow.keras.backend import clear_session

gkf = GroupKFold()

"""# **Preprocesamiento**"""

all_files=glob('/content/F_Imag/CMD_DATA/*.csv') + glob('/content/F_Imag/DES_DATA/*.csv')

Descanso_path=[i for i in all_files if ('M4' in i.split('/')[-1] or 'I4' in i.split('/')[-1])]
RCH_path=[i for i in all_files if 'M3' in i.split('/')[-1]]

print(len(all_files))
print(len(Descanso_path))
print(len(RCH_path))

"""Aumento de datos"""

frame_size = 250 # Tamaño de ventana temporal
step = 65 # Paso de desplazamiento
for i in range(0,500-frame_size+1,step):
  print(i,i+frame_size)

def get_frame(file,frame_size,step): 
  df = pd.read_csv(file)
  column_list = df.columns
  if len(column_list) == 9:
    df = pd.read_csv(file).iloc[:,1:]
  else:
    df = pd.read_csv(file).iloc[:,2:]
    df = df.rename(columns={'1':'0','2':'1','3':'2','4':'3','5':'4','6':'5',\
                            '7':'6','8':'7'})
  frames=[]  
  min_max_scaler=preprocessing.MinMaxScaler()

  for i in range(0,len(df)-frame_size+1,step):
    ch1=min_max_scaler.fit_transform(df['0'].values[i:i+frame_size].reshape(frame_size,1))
    ch2=min_max_scaler.fit_transform(df['1'].values[i:i+frame_size].reshape(frame_size,1))
    ch3=min_max_scaler.fit_transform(df['2'].values[i:i+frame_size].reshape(frame_size,1))
    ch4=min_max_scaler.fit_transform(df['3'].values[i:i+frame_size].reshape(frame_size,1))
    ch5=min_max_scaler.fit_transform(df['4'].values[i:i+frame_size].reshape(frame_size,1))
    ch6=min_max_scaler.fit_transform(df['5'].values[i:i+frame_size].reshape(frame_size,1))
    ch7=min_max_scaler.fit_transform(df['6'].values[i:i+frame_size].reshape(frame_size,1))
    ch8=min_max_scaler.fit_transform(df['7'].values[i:i+frame_size].reshape(frame_size,1))
    
    array = np.concatenate((ch2,ch6,ch1,ch5,ch7,ch4), axis=1)
    frames.append(array)
  return np.array(frames)

Descanso_array = [get_frame(i,frame_size,step) for i in Descanso_path]
RCH_array = [get_frame(i,frame_size,step) for i in RCH_path]

Descanso_labels=[len(i)*[0] for i in Descanso_array]
RCH_labels=[len(i)*[1] for i in RCH_array]

data_list = Descanso_array + RCH_array 
label_list = Descanso_labels + RCH_labels
print(len(data_list),len(label_list))

"""Seleccion de datos para test"""

import random
datos_test = []
labels_test = []
target_cmd= 0
target_des = 0
while len(datos_test) !=250:
  ind = random.randint(0,len(data_list))

  if label_list[ind][0] == 0 and target_des != 125:
    datos_test.append(data_list[ind])
    labels_test.append(label_list[ind])
    data_list.pop(ind)
    label_list.pop(ind)
    target_des=1 + target_des
    print("0")

  elif label_list[ind][0] == 1 and target_cmd != 125:
    datos_test.append(data_list[ind])
    labels_test.append(label_list[ind])
    data_list.pop(ind)
    label_list.pop(ind)
    target_cmd =1+target_cmd
    print("1")

target_des,target_des

data_array = np.vstack(data_list)
label_array=np.hstack(label_list)

data_test = np.vstack(datos_test)
label_test = np.hstack(labels_test)

print(data_array.shape,label_array.shape)
print(data_test.shape,label_test.shape)

dimension = data_array.shape[1:]
print(dimension)

"""# **Modelo de red**"""

def cnnmodel():
  clear_session()
  
  model = Sequential()

  model.add(Conv1D(filters=64,kernel_size=40,strides=1,activation = "relu",padding="same",input_shape=dimension))
  model.add(BatchNormalization())
  model.add(tf.keras.layers.SpatialDropout1D(0.5)) 
  model.add(tf.keras.layers.LeakyReLU(alpha=0.3))
  
  model.add(Conv1D(filters=64,kernel_size=40,activation = "relu",strides=1,padding="valid"))
  model.add(BatchNormalization())
  model.add(tf.keras.layers.SpatialDropout1D(0.5)) 
  
  model.add(Conv1D(filters=32,kernel_size=40,activation = "relu",strides=1,padding="valid"))
  model.add(BatchNormalization())
  model.add(tf.keras.layers.SpatialDropout1D(0.5)) 
  

  model.add(Conv1D(filters=32,kernel_size=40,activation = "relu",strides=1,padding="valid"))
  model.add(AveragePooling1D(pool_size=2))
  model.add(tf.keras.layers.SpatialDropout1D(0.5)) 

  model.add(Flatten())
  model.add(Dense(250,activation="relu"))
  model.add(Dropout(0.2))

  model.add(Dense(100,activation="relu"))
  model.add(Dropout(0.2))
  model.add(Dense(1,activation="sigmoid"))
  opt = keras.optimizers.Adam(learning_rate=0.0001)
 
 model.compile(optimizer=opt,loss="binary_crossentropy",metrics=["accuracy",keras.metrics.Precision(), keras.metrics.Recall()])
  return model

model=cnnmodel()
model.summary()

"""# **Entrenamiento**"""

X_train, X_val, y_train, y_val = train_test_split(data_array, label_array, test_size=0.2,random_state=15)
print(data_array.shape,label_array.shape)
print(X_train.shape,y_train.shape)
print(X_val.shape,y_val.shape)

start_time = time.time()
print("--- %s seconds ---" % (time.time() - start_time))

accuracy = []
loss=[]
keras.backend.clear_session()

model=cnnmodel()

start_time = time.time()

history = model.fit(X_train,y_train,epochs=300,batch_size=32,validation_data=(X_val,y_val))
print("Tiempo de entrenamiento:", (time.time() - start_time))

accuracy.append(model.evaluate(X_val,y_val)[1])
loss.append(model.evaluate(X_val,y_val)[0])

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title(" Exactitud del Modelo" )
plt.ylabel('Exactitud')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.savefig("Accuracy_train_.png",bbox_inches="tight")
plt.show()

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Perdidas del Modelo ' )
plt.ylabel('Perdida')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper right')
plt.savefig("Loss_train_.png",bbox_inches="tight")
plt.show()

"""## Matriz Test"""

model.evaluate(data_test,label_test)
from sklearn.metrics import confusion_matrix

#Predict
y_prediction = model.predict(data_test)
y_pred = [1 * (x[0]>0.45) for x in y_prediction]

#Create confusion matrix and normalizes it over predicted (columns)
print(type(y_pred))
print(type(label_test))

result = confusion_matrix(label_test, np.array(y_pred))

print(result)

import seaborn as sns

ax = sns.heatmap(result/np.sum(result), annot=True, 
            fmt='.2%', cmap='Blues')

ax.set_title('Matriz de confusión\n\n');
ax.set_xlabel('\nValores de Predicción')
ax.set_ylabel('Valores Reales ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['Descanso','Cerrar Mano'])
ax.yaxis.set_ticklabels(['Descanso','Cerrar Mano'])

## Display the visualization of the Confusion Matrix.
plt.savefig('MatrizConfusion50_test_individual.png',bbox_inches="tight")
plt.show()

print(np.array(y_pred))
print(label_test)

"""## Test"""

from statistics import mean
print(labels_test)
test_labels = []
for i in labels_test:
  test_labels.append(mean(i))
print(test_labels)

import numpy as np
test_predict = []
for i in datos_test:
  y_prediction = model.predict(i)
  prom_pred=[]
  #print(y_prediction)
  for i in y_prediction:
      norm = 1 * (i > 0.60)
      
      prom_pred.append(norm)
  
  #print(prom_pred)
  predict = np.mean(prom_pred)
  norm_predict = 1 * (predict > 0.5)
  test_predict.append(norm_predict)
  #print(norm)
print(test_predict)
print(test_labels)

result = confusion_matrix(test_predict, test_labels)
print(result)

ax = sns.heatmap(result/np.sum(result), annot=True, 
            fmt='.2%', cmap='Blues')

ax.set_title('Matriz de confusión\n\n');
ax.set_xlabel('\nValores de Predicción')
ax.set_ylabel('Valores Reales ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['Descanso','Cerrar Mano'])
ax.yaxis.set_ticklabels(['Descanso','Cerrar Mano'])

## Display the visualization of the Confusion Matrix.
plt.savefig('MatrizConfusion50_test.png',bbox_inches="tight")

plt.show()

"""# **Guardar Modelo**"""

model.save("Modelo_acc_58%.h5")

"""# **K FOLD**"""

from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold
from sklearn.utils import shuffle
from sklearn.model_selection import StratifiedShuffleSplit

scores= []
accuracy_val= []
loss_val= []
accuracy_train= []
loss_train= []

def get_score(model, X_train, X_val, y_train, y_val, K):
    history = model.fit(X_train,y_train,epochs=300,batch_size=32,validation_data=(X_val,y_val))
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title(' Exactitud del Modelo ( K = '+ str(K) +" )" )
    plt.ylabel('Exactitud')
    plt.xlabel('epoch')
    plt.legend(['train', 'val'], loc='upper left')
    plt.savefig("/content/KFold/Accuracy_"+ str(K) +"_.png",bbox_inches="tight")
    plt.show()

    # summarize history for loss
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Perdidas del Modelo ( K = '+ str(K) +" )" )
    plt.ylabel('Perdida')
    plt.xlabel('epoch')
    plt.legend(['train', 'val'], loc='upper right')
    plt.savefig("/content/KFold/Loss_"+ str(K) +"_.png",bbox_inches="tight")
    plt.show()
    
    accuracy_val.append(model.evaluate(X_val,y_val)[1])
    loss_val.append(model.evaluate(X_val,y_val)[0])
    accuracy_train.append(model.evaluate(X_train,y_train)[1])
    loss_train.append(model.evaluate(X_train,y_train)[0])
    return model.evaluate(X_val,y_val)[1]   

folds = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)
K=1
for train_index, val_index in folds.split(data_array,label_array):
    keras.backend.clear_session()

    model=cnnmodel()
    print(train_index,val_index)
    X= shuffle(train_index ,random_state=0)
    Y=shuffle(val_index ,random_state=0)
    X_train, X_val, y_train, y_val = data_array[train_index], data_array[val_index], \
                                      label_array[train_index], label_array[val_index]
    
    scores.append(get_score(model, X_train, X_val, y_train, y_val,K)) 
    K = 1 + K

print(mean(accuracy_val),
mean(loss_val),
mean(accuracy_train),
mean(loss_train))

model.summary()